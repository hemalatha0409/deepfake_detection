{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":16880,"databundleVersionId":858837,"sourceType":"competition"},{"sourceId":924245,"sourceType":"datasetVersion","datasetId":464091}],"dockerImageVersionId":29845,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Deep Fake Detection using CNN and RNN","metadata":{"id":"Q6tasuafvT2O"}},{"cell_type":"markdown","source":"# Contriubuters - ","metadata":{}},{"cell_type":"markdown","source":"## 1. Rohan Inamdar\n## 2. Kavin Sundarr","metadata":{}},{"cell_type":"markdown","source":"## Importing Required libraries","metadata":{"id":"dFXIv9qNpKzt","tags":[]}},{"cell_type":"code","source":"!pip install -U --upgrade tensorflow","metadata":{"execution":{"iopub.status.busy":"2024-04-14T19:41:20.273475Z","iopub.execute_input":"2024-04-14T19:41:20.273753Z","iopub.status.idle":"2024-04-14T19:42:49.706610Z","shell.execute_reply.started":"2024-04-14T19:41:20.273709Z","shell.execute_reply":"2024-04-14T19:42:49.705933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sys\nimport sklearn\nimport tensorflow as tf\n\nimport cv2\nimport pandas as pd\nimport numpy as np\n\nimport plotly.graph_objs as go\nfrom plotly.offline import iplot\nfrom matplotlib import pyplot as plt","metadata":{"id":"TFSU3FCOpKzu","execution":{"iopub.status.busy":"2024-04-14T19:43:04.131661Z","iopub.execute_input":"2024-04-14T19:43:04.132072Z","iopub.status.idle":"2024-04-14T19:43:07.962849Z","shell.execute_reply.started":"2024-04-14T19:43:04.131999Z","shell.execute_reply":"2024-04-14T19:43:07.962085Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.test.is_gpu_available()","metadata":{"execution":{"iopub.status.busy":"2024-04-14T19:43:15.250181Z","iopub.execute_input":"2024-04-14T19:43:15.250524Z","iopub.status.idle":"2024-04-14T19:43:15.406249Z","shell.execute_reply.started":"2024-04-14T19:43:15.250463Z","shell.execute_reply":"2024-04-14T19:43:15.405202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.__version__","metadata":{"execution":{"iopub.status.busy":"2024-04-14T19:43:21.456204Z","iopub.execute_input":"2024-04-14T19:43:21.456497Z","iopub.status.idle":"2024-04-14T19:43:21.461660Z","shell.execute_reply.started":"2024-04-14T19:43:21.456454Z","shell.execute_reply":"2024-04-14T19:43:21.460899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nplt.rc('font', size=14)\nplt.rc('axes', labelsize=14, titlesize=14)\nplt.rc('legend', fontsize=14)\nplt.rc('xtick', labelsize=10)\nplt.rc('ytick', labelsize=10)","metadata":{"id":"8d4TH3NbpKzx","execution":{"iopub.status.busy":"2024-04-14T19:43:25.460649Z","iopub.execute_input":"2024-04-14T19:43:25.460943Z","iopub.status.idle":"2024-04-14T19:43:25.466753Z","shell.execute_reply.started":"2024-04-14T19:43:25.460889Z","shell.execute_reply":"2024-04-14T19:43:25.465795Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Visualisation","metadata":{"id":"NL3Ht4wC9b3n"}},{"cell_type":"code","source":"import os\n\ndef get_data():\n    return pd.read_csv('../input/deepfake-faces/metadata.csv')","metadata":{"id":"jfv9PxSB4tM8","execution":{"iopub.status.busy":"2024-04-14T19:43:30.480984Z","iopub.execute_input":"2024-04-14T19:43:30.481410Z","iopub.status.idle":"2024-04-14T19:43:30.486004Z","shell.execute_reply.started":"2024-04-14T19:43:30.481326Z","shell.execute_reply":"2024-04-14T19:43:30.484943Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"meta=get_data()\nmeta.head()","metadata":{"id":"tDW7BRph9ehF","outputId":"97de18b5-0a37-4302-8804-8a16a7d2ed2f","execution":{"iopub.status.busy":"2024-04-14T19:43:37.766026Z","iopub.execute_input":"2024-04-14T19:43:37.766310Z","iopub.status.idle":"2024-04-14T19:43:37.956349Z","shell.execute_reply.started":"2024-04-14T19:43:37.766268Z","shell.execute_reply":"2024-04-14T19:43:37.955654Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"meta.shape","metadata":{"id":"n7FSdDifbZxn","outputId":"5451a127-405a-4c0b-a197-c920b796adbb","execution":{"iopub.status.busy":"2024-04-14T19:43:41.820867Z","iopub.execute_input":"2024-04-14T19:43:41.821182Z","iopub.status.idle":"2024-04-14T19:43:41.826422Z","shell.execute_reply.started":"2024-04-14T19:43:41.821129Z","shell.execute_reply":"2024-04-14T19:43:41.825401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(meta[meta.label=='FAKE']),len(meta[meta.label=='REAL'])","metadata":{"id":"_FJcz2IthxVG","outputId":"274c3f65-7acb-4f99-8aa9-a5b2a23bf06a","execution":{"iopub.status.busy":"2024-04-14T19:43:45.391272Z","iopub.execute_input":"2024-04-14T19:43:45.391820Z","iopub.status.idle":"2024-04-14T19:43:45.436328Z","shell.execute_reply.started":"2024-04-14T19:43:45.391558Z","shell.execute_reply":"2024-04-14T19:43:45.435495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"real_df = meta[meta[\"label\"] == \"REAL\"]\nfake_df = meta[meta[\"label\"] == \"FAKE\"]\nsample_size = 8000\n\nreal_df = real_df.sample(sample_size, random_state=42)\nfake_df = fake_df.sample(sample_size, random_state=42)\n\nsample_meta = pd.concat([real_df, fake_df])","metadata":{"id":"IgMfzY-PjjtH","execution":{"iopub.status.busy":"2024-04-14T19:43:47.776128Z","iopub.execute_input":"2024-04-14T19:43:47.776414Z","iopub.status.idle":"2024-04-14T19:43:47.826546Z","shell.execute_reply.started":"2024-04-14T19:43:47.776374Z","shell.execute_reply":"2024-04-14T19:43:47.825668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As mentioned instead of using 95k images we will only use 16000 images.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nTrain_set, Test_set = train_test_split(sample_meta,test_size=0.2,random_state=42,stratify=sample_meta['label'])\nTrain_set, Val_set  = train_test_split(Train_set,test_size=0.3,random_state=42,stratify=Train_set['label'])","metadata":{"id":"5eB86S6K-T5Z","execution":{"iopub.status.busy":"2024-04-14T19:43:56.980788Z","iopub.execute_input":"2024-04-14T19:43:56.981097Z","iopub.status.idle":"2024-04-14T19:43:57.594348Z","shell.execute_reply.started":"2024-04-14T19:43:56.981053Z","shell.execute_reply":"2024-04-14T19:43:57.593634Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Train_set.shape,Val_set.shape,Test_set.shape","metadata":{"id":"8p-TONijb4qA","outputId":"56d0b529-9d81-4019-d8fa-618c8cdba90f","execution":{"iopub.status.busy":"2024-04-14T19:44:01.041181Z","iopub.execute_input":"2024-04-14T19:44:01.041478Z","iopub.status.idle":"2024-04-14T19:44:01.054899Z","shell.execute_reply.started":"2024-04-14T19:44:01.041435Z","shell.execute_reply":"2024-04-14T19:44:01.053982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y = dict()\n\ny[0] = []\ny[1] = []\n\nfor set_name in (np.array(Train_set['label']), np.array(Val_set['label']), np.array(Test_set['label'])):\n    y[0].append(np.sum(set_name == 'REAL'))\n    y[1].append(np.sum(set_name == 'FAKE'))\n\ntrace0 = go.Bar(\n    x=['Train Set', 'Validation Set', 'Test Set'],\n    y=y[0],\n    name='REAL',\n    marker=dict(color='#33cc33'),\n    opacity=0.7\n)\ntrace1 = go.Bar(\n    x=['Train Set', 'Validation Set', 'Test Set'],\n    y=y[1],\n    name='FAKE',\n    marker=dict(color='#ff3300'),\n    opacity=0.7\n)\n\ndata = [trace0, trace1]\nlayout = go.Layout(\n    title='Count of classes in each set',\n    xaxis={'title': 'Set'},\n    yaxis={'title': 'Count'}\n)\n\nfig = go.Figure(data, layout)\niplot(fig)","metadata":{"id":"hzNGtCWd-mTk","outputId":"5178c3ed-cbba-4f99-99d0-26bde11a5dab","execution":{"iopub.status.busy":"2024-04-14T19:44:05.463881Z","iopub.execute_input":"2024-04-14T19:44:05.464208Z","iopub.status.idle":"2024-04-14T19:44:06.716732Z","shell.execute_reply.started":"2024-04-14T19:44:05.464162Z","shell.execute_reply":"2024-04-14T19:44:06.715772Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The original image dataset were biased with more fake images than real since we are taking a sample of it its better to take equal proportion of real and fake images.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(15,15))\nfor cur,i in enumerate(Train_set.index[25:50]):\n    plt.subplot(5,5,cur+1)\n    plt.xticks([])\n    plt.yticks([])\n    plt.grid(False)\n    \n    plt.imshow(cv2.imread('../input/deepfake-faces/faces_224/'+Train_set.loc[i,'videoname'][:-4]+'.jpg'))\n    \n    if(Train_set.loc[i,'label']=='FAKE'):\n        plt.xlabel('FAKE Image')\n    else:\n        plt.xlabel('REAL Image')\n        \nplt.show()","metadata":{"id":"VR7Uly2fcUYi","outputId":"c1f47a82-ef4f-4bcd-b51c-d4738142fc0f","execution":{"iopub.status.busy":"2024-04-14T19:44:23.301439Z","iopub.execute_input":"2024-04-14T19:44:23.301767Z","iopub.status.idle":"2024-04-14T19:44:25.533713Z","shell.execute_reply.started":"2024-04-14T19:44:23.301719Z","shell.execute_reply":"2024-04-14T19:44:25.532964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Base Model","metadata":{"id":"dOvN_divkl-N"}},{"cell_type":"markdown","source":"### Custom CNN Architecture","metadata":{"id":"oid44Xx-pKz6"}},{"cell_type":"code","source":"def retreive_dataset(set_name):\n    images,labels=[],[]\n    for (img, imclass) in zip(set_name['videoname'], set_name['label']):\n        images.append(cv2.imread('../input/deepfake-faces/faces_224/'+img[:-4]+'.jpg'))\n        if(imclass=='FAKE'):\n            labels.append(1)\n        else:\n            labels.append(0)\n    \n    return np.array(images),np.array(labels)","metadata":{"id":"Hz0ZdQ_fgHhG","execution":{"iopub.status.busy":"2024-04-14T19:44:34.816221Z","iopub.execute_input":"2024-04-14T19:44:34.816520Z","iopub.status.idle":"2024-04-14T19:44:34.823545Z","shell.execute_reply.started":"2024-04-14T19:44:34.816477Z","shell.execute_reply":"2024-04-14T19:44:34.822602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train,y_train=retreive_dataset(Train_set)\nX_val,y_val=retreive_dataset(Val_set)\nX_test,y_test=retreive_dataset(Test_set)","metadata":{"id":"zeAGRcAbguKU","execution":{"iopub.status.busy":"2024-04-14T19:44:40.617069Z","iopub.execute_input":"2024-04-14T19:44:40.617402Z","iopub.status.idle":"2024-04-14T19:47:22.446391Z","shell.execute_reply.started":"2024-04-14T19:44:40.617341Z","shell.execute_reply":"2024-04-14T19:47:22.445718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from functools import partial\n\ntf.random.set_seed(42) \nDefaultConv2D = partial(tf.keras.layers.Conv2D, kernel_size=3, padding=\"same\",\n                        activation=\"relu\", kernel_initializer=\"he_normal\")\n\nmodel = tf.keras.Sequential([\n    DefaultConv2D(filters=64, kernel_size=7, input_shape=[224, 224, 3]),\n    tf.keras.layers.MaxPool2D(),\n    DefaultConv2D(filters=128),\n    DefaultConv2D(filters=128),\n    tf.keras.layers.MaxPool2D(),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(units=128, activation=\"relu\",\n                          kernel_initializer=\"he_normal\"),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Dense(units=64, activation=\"relu\",\n                          kernel_initializer=\"he_normal\"),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Dense(units=1, activation=\"sigmoid\")\n])","metadata":{"id":"34upiak4pKz6","execution":{"iopub.status.busy":"2024-04-14T19:48:25.426047Z","iopub.execute_input":"2024-04-14T19:48:25.426360Z","iopub.status.idle":"2024-04-14T19:48:26.362798Z","shell.execute_reply.started":"2024-04-14T19:48:25.426316Z","shell.execute_reply":"2024-04-14T19:48:26.362167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(loss=\"binary_crossentropy\", optimizer=\"nadam\",\n              metrics=[\"accuracy\"])\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2024-04-14T19:48:29.551203Z","iopub.execute_input":"2024-04-14T19:48:29.551490Z","iopub.status.idle":"2024-04-14T19:48:29.579045Z","shell.execute_reply.started":"2024-04-14T19:48:29.551448Z","shell.execute_reply":"2024-04-14T19:48:29.577839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit(X_train, y_train, epochs=5,batch_size=64,\n                    validation_data=(X_val, y_val))","metadata":{"id":"KZbWeIBYpKz6","outputId":"deb6f56a-7b93-4241-a1bd-b210c0f2d426","execution":{"iopub.status.busy":"2024-04-14T19:48:34.715975Z","iopub.execute_input":"2024-04-14T19:48:34.716284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"score = model.evaluate(X_test, y_test)","metadata":{"id":"6HDDr4uehast","execution":{"iopub.status.busy":"2022-07-09T07:49:17.25106Z","iopub.execute_input":"2022-07-09T07:49:17.251412Z","iopub.status.idle":"2022-07-09T07:49:23.443216Z","shell.execute_reply.started":"2022-07-09T07:49:17.251376Z","shell.execute_reply":"2022-07-09T07:49:23.442158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot model performance\nacc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs_range = range(1, len(history.epoch) + 1)\n\nplt.figure(figsize=(15,5))\n\nplt.subplot(1, 2, 1)\nplt.plot(epochs_range, acc, label='Train Set')\nplt.plot(epochs_range, val_acc, label='Val Set')\nplt.legend(loc=\"best\")\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.title('Model Accuracy')\n\nplt.subplot(1, 2, 2)\nplt.plot(epochs_range, loss, label='Train Set')\nplt.plot(epochs_range, val_loss, label='Val Set')\nplt.legend(loc=\"best\")\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('Model Loss')\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-09T07:49:49.846532Z","iopub.execute_input":"2022-07-09T07:49:49.846938Z","iopub.status.idle":"2022-07-09T07:49:50.222822Z","shell.execute_reply.started":"2022-07-09T07:49:49.846884Z","shell.execute_reply":"2022-07-09T07:49:50.221805Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**A baseline score is set here around ~51%**","metadata":{}},{"cell_type":"markdown","source":"# Pretrained Models for Transfer Learning","metadata":{"id":"hqxnSBJ3pKz8"}},{"cell_type":"markdown","source":"using Xception model for fine-tuning ","metadata":{}},{"cell_type":"code","source":"train_set_raw=tf.data.Dataset.from_tensor_slices((X_train,y_train))\nvalid_set_raw=tf.data.Dataset.from_tensor_slices((X_val,y_val))\ntest_set_raw=tf.data.Dataset.from_tensor_slices((X_test,y_test))","metadata":{"execution":{"iopub.status.busy":"2022-07-09T07:50:26.252046Z","iopub.execute_input":"2022-07-09T07:50:26.253267Z","iopub.status.idle":"2022-07-09T07:50:30.591156Z","shell.execute_reply.started":"2022-07-09T07:50:26.253219Z","shell.execute_reply":"2022-07-09T07:50:30.590137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.keras.backend.clear_session()  # extra code – resets layer name counter\n\nbatch_size = 32\npreprocess = tf.keras.applications.xception.preprocess_input\ntrain_set = train_set_raw.map(lambda X, y: (preprocess(tf.cast(X, tf.float32)), y))\ntrain_set = train_set.shuffle(1000, seed=42).batch(batch_size).prefetch(1)\nvalid_set = valid_set_raw.map(lambda X, y: (preprocess(tf.cast(X, tf.float32)), y)).batch(batch_size)\ntest_set = test_set_raw.map(lambda X, y: (preprocess(tf.cast(X, tf.float32)), y)).batch(batch_size)","metadata":{"id":"Bnz0n9XApKz9","execution":{"iopub.status.busy":"2022-07-09T07:50:32.37774Z","iopub.execute_input":"2022-07-09T07:50:32.378399Z","iopub.status.idle":"2022-07-09T07:50:32.485208Z","shell.execute_reply.started":"2022-07-09T07:50:32.378362Z","shell.execute_reply":"2022-07-09T07:50:32.484269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# extra code – displays the first 9 images in the first batch of valid_set\n\nplt.figure(figsize=(12, 12))\nfor X_batch, y_batch in valid_set.take(1):\n    for index in range(9):\n        plt.subplot(3, 3, index + 1)\n        plt.imshow((X_batch[index] + 1) / 2)  # rescale to 0–1 for imshow()\n        if(y_batch[index]==1):\n            classt='FAKE'\n        else:\n            classt='REAL'\n        plt.title(f\"Class: {classt}\")\n        plt.axis(\"off\")\n\nplt.show()","metadata":{"id":"ZL3c3i4opKz9","outputId":"38847d8d-8822-41a3-cfb2-27479aa5debe","execution":{"iopub.status.busy":"2022-07-09T07:50:36.117357Z","iopub.execute_input":"2022-07-09T07:50:36.117715Z","iopub.status.idle":"2022-07-09T07:50:37.253752Z","shell.execute_reply.started":"2022-07-09T07:50:36.117686Z","shell.execute_reply":"2022-07-09T07:50:37.25288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_augmentation = tf.keras.Sequential([\n    tf.keras.layers.RandomFlip(mode=\"horizontal\", seed=42),\n    tf.keras.layers.RandomRotation(factor=0.05, seed=42),\n    tf.keras.layers.RandomContrast(factor=0.2, seed=42)\n])","metadata":{"id":"Ib0cA8Y1pKz9","execution":{"iopub.status.busy":"2022-07-09T07:50:55.607639Z","iopub.execute_input":"2022-07-09T07:50:55.608024Z","iopub.status.idle":"2022-07-09T07:50:55.628959Z","shell.execute_reply.started":"2022-07-09T07:50:55.60799Z","shell.execute_reply":"2022-07-09T07:50:55.627934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# extra code – displays the same first 9 images, after augmentation\n\nplt.figure(figsize=(12, 12))\nfor X_batch, y_batch in valid_set.take(1):\n    X_batch_augmented = data_augmentation(X_batch, training=True)\n    for index in range(9):\n        plt.subplot(3, 3, index + 1)\n        # We must rescale the images to the 0-1 range for imshow(), and also\n        # clip the result to that range, because data augmentation may\n        # make some values go out of bounds (e.g., RandomContrast in this case).\n        plt.imshow(np.clip((X_batch_augmented[index] + 1) / 2, 0, 1))\n        if(y_batch[index]==1):\n            classt='FAKE'\n        else:\n            classt='REAL'\n        plt.title(f\"Class: {classt}\")\n        plt.axis(\"off\")\n\nplt.show()","metadata":{"id":"w6GH5_vupKz-","outputId":"eeb2c924-2f4f-4aa1-bea9-951bebef4bf0","execution":{"iopub.status.busy":"2022-07-09T07:50:58.415776Z","iopub.execute_input":"2022-07-09T07:50:58.416464Z","iopub.status.idle":"2022-07-09T07:50:59.639178Z","shell.execute_reply.started":"2022-07-09T07:50:58.416417Z","shell.execute_reply":"2022-07-09T07:50:59.638315Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.random.set_seed(42)  # extra code – ensures reproducibility\nbase_model = tf.keras.applications.xception.Xception(weights=\"imagenet\",\n                                                     include_top=False)\navg = tf.keras.layers.GlobalAveragePooling2D()(base_model.output)\noutput = tf.keras.layers.Dense(1, activation=\"sigmoid\")(avg)\nmodel = tf.keras.Model(inputs=base_model.input, outputs=output)","metadata":{"id":"lRyCgvaKpKz-","outputId":"a825e173-8b1d-4217-a1c4-5491b49c3e82","execution":{"iopub.status.busy":"2022-07-09T07:51:03.816043Z","iopub.execute_input":"2022-07-09T07:51:03.816392Z","iopub.status.idle":"2022-07-09T07:51:05.745785Z","shell.execute_reply.started":"2022-07-09T07:51:03.816363Z","shell.execute_reply":"2022-07-09T07:51:05.744827Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for layer in base_model.layers:\n    layer.trainable = False","metadata":{"id":"KBlyG6ElpKz-","execution":{"iopub.status.busy":"2022-07-09T07:51:09.109193Z","iopub.execute_input":"2022-07-09T07:51:09.109549Z","iopub.status.idle":"2022-07-09T07:51:09.118356Z","shell.execute_reply.started":"2022-07-09T07:51:09.109518Z","shell.execute_reply":"2022-07-09T07:51:09.117183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimizer = tf.keras.optimizers.SGD(learning_rate=0.1, momentum=0.9)\nmodel.compile(loss=\"binary_crossentropy\", optimizer=optimizer,\n              metrics=[\"accuracy\"])\nhistory = model.fit(train_set, validation_data=valid_set, epochs=3)","metadata":{"id":"GGxK2yPcpKz-","outputId":"6b64214a-e104-4b6c-9b7a-3388fc9aa15f","execution":{"iopub.status.busy":"2022-07-09T07:51:16.497154Z","iopub.execute_input":"2022-07-09T07:51:16.497852Z","iopub.status.idle":"2022-07-09T07:53:16.98367Z","shell.execute_reply.started":"2022-07-09T07:51:16.497818Z","shell.execute_reply":"2022-07-09T07:53:16.982649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for indices in zip(range(33), range(33, 66), range(66, 99), range(99, 132)):\n    for idx in indices:\n        print(f\"{idx:3}: {base_model.layers[idx].name:22}\", end=\"\")\n    print()","metadata":{"id":"GvGMiJMLpKz-","outputId":"91f2c96c-c058-45e0-e428-66fa6076ad56","execution":{"iopub.status.busy":"2022-07-09T07:53:19.291082Z","iopub.execute_input":"2022-07-09T07:53:19.291655Z","iopub.status.idle":"2022-07-09T07:53:19.34738Z","shell.execute_reply.started":"2022-07-09T07:53:19.291619Z","shell.execute_reply":"2022-07-09T07:53:19.343342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.evaluate(test_set)","metadata":{"execution":{"iopub.status.busy":"2022-07-09T07:53:24.357615Z","iopub.execute_input":"2022-07-09T07:53:24.357978Z","iopub.status.idle":"2022-07-09T07:53:32.546789Z","shell.execute_reply.started":"2022-07-09T07:53:24.357946Z","shell.execute_reply":"2022-07-09T07:53:32.545927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**After Fine tuning The accuracy comes about ~64%**","metadata":{}},{"cell_type":"markdown","source":"Now that the weights of our new top layers are not too bad, we can make the top part of the base model trainable again, and continue training, but with a lower learning rate:","metadata":{"id":"L_bEwL8KpKz_"}},{"cell_type":"code","source":"for layer in base_model.layers[56:]:\n    layer.trainable = True\n\noptimizer = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9)\nmodel.compile(loss=\"binary_crossentropy\", optimizer=optimizer,\n              metrics=[\"accuracy\"])\nhistory = model.fit(train_set, validation_data=valid_set, epochs=10)","metadata":{"id":"GEUNGlhvpKz_","outputId":"c622a91d-f634-4443-b87e-8d46defdb578","execution":{"iopub.status.busy":"2022-07-09T07:54:22.042837Z","iopub.execute_input":"2022-07-09T07:54:22.043219Z","iopub.status.idle":"2022-07-09T08:07:10.55012Z","shell.execute_reply.started":"2022-07-09T07:54:22.043188Z","shell.execute_reply":"2022-07-09T08:07:10.549045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot model performance\nacc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs_range = range(1, len(history.epoch) + 1)\n\nplt.figure(figsize=(15,5))\n\nplt.subplot(1, 2, 1)\nplt.plot(epochs_range, acc, label='Train Set')\nplt.plot(epochs_range, val_acc, label='Val Set')\nplt.legend(loc=\"best\")\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.title('Model Accuracy')\n\nplt.subplot(1, 2, 2)\nplt.plot(epochs_range, loss, label='Train Set')\nplt.plot(epochs_range, val_loss, label='Val Set')\nplt.legend(loc=\"best\")\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('Model Loss')\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-09T08:07:10.552574Z","iopub.execute_input":"2022-07-09T08:07:10.553267Z","iopub.status.idle":"2022-07-09T08:07:10.897626Z","shell.execute_reply.started":"2022-07-09T08:07:10.553227Z","shell.execute_reply":"2022-07-09T08:07:10.896726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.evaluate(test_set)","metadata":{"execution":{"iopub.status.busy":"2022-07-09T08:07:10.899017Z","iopub.execute_input":"2022-07-09T08:07:10.899513Z","iopub.status.idle":"2022-07-09T08:07:21.780753Z","shell.execute_reply.started":"2022-07-09T08:07:10.899472Z","shell.execute_reply":"2022-07-09T08:07:21.779791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**The model accuracy finally reaches to 81.9%**","metadata":{}},{"cell_type":"code","source":"model.save('xception_deepfake_image.h5')","metadata":{"execution":{"iopub.status.busy":"2022-07-09T08:07:21.782619Z","iopub.execute_input":"2022-07-09T08:07:21.783655Z","iopub.status.idle":"2022-07-09T08:07:22.273765Z","shell.execute_reply.started":"2022-07-09T08:07:21.783609Z","shell.execute_reply":"2022-07-09T08:07:22.272813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Adding Explainability to the model","metadata":{}},{"cell_type":"code","source":"!pip install lime","metadata":{"execution":{"iopub.status.busy":"2022-07-09T08:07:36.254591Z","iopub.execute_input":"2022-07-09T08:07:36.255002Z","iopub.status.idle":"2022-07-09T08:07:47.12565Z","shell.execute_reply.started":"2022-07-09T08:07:36.254957Z","shell.execute_reply":"2022-07-09T08:07:47.124428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from lime import lime_image\n\nexplainer = lime_image.LimeImageExplainer()","metadata":{"execution":{"iopub.status.busy":"2022-07-09T08:07:51.014667Z","iopub.execute_input":"2022-07-09T08:07:51.015082Z","iopub.status.idle":"2022-07-09T08:07:52.040583Z","shell.execute_reply.started":"2022-07-09T08:07:51.015046Z","shell.execute_reply":"2022-07-09T08:07:52.03955Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12, 12))\n\nfor index in range(9):\n    plt.subplot(3, 3, index + 1)\n    plt.imshow((x[index] + 1) / 2)  # rescale to 0–1 for imshow()\n    if(y[index]==1):\n        classt='FAKE'\n    else:\n        classt='REAL'\n    plt.title(f\"Class: {classt}\")\n    plt.axis(\"off\")\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-09T08:08:00.18824Z","iopub.execute_input":"2022-07-09T08:08:00.188589Z","iopub.status.idle":"2022-07-09T08:08:01.16772Z","shell.execute_reply.started":"2022-07-09T08:08:00.188559Z","shell.execute_reply":"2022-07-09T08:08:01.166813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data=x[2,:,:,:]\ntest_data.shape","metadata":{"execution":{"iopub.status.busy":"2022-07-09T08:08:09.212978Z","iopub.execute_input":"2022-07-09T08:08:09.213858Z","iopub.status.idle":"2022-07-09T08:08:09.220711Z","shell.execute_reply.started":"2022-07-09T08:08:09.213821Z","shell.execute_reply":"2022-07-09T08:08:09.219615Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"explanation = explainer.explain_instance(test_data.astype('double'), model.predict,  \n                                         top_labels=3, hide_color=0, num_samples=1000)","metadata":{"execution":{"iopub.status.busy":"2022-07-09T08:08:12.876371Z","iopub.execute_input":"2022-07-09T08:08:12.87672Z","iopub.status.idle":"2022-07-09T08:08:25.485758Z","shell.execute_reply.started":"2022-07-09T08:08:12.876688Z","shell.execute_reply":"2022-07-09T08:08:25.484661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from skimage.segmentation import mark_boundaries\n\ntemp_1, mask_1 = explanation.get_image_and_mask(explanation.top_labels[0], positive_only=True, num_features=5, hide_rest=True)\ntemp_2, mask_2 = explanation.get_image_and_mask(explanation.top_labels[0], positive_only=False, num_features=10, hide_rest=False)\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15,15))\nax1.imshow(mark_boundaries(temp_1, mask_1))\nax2.imshow(mark_boundaries(temp_2, mask_2))\nax1.axis('off')\nax2.axis('off')","metadata":{"execution":{"iopub.status.busy":"2022-07-09T08:08:34.634728Z","iopub.execute_input":"2022-07-09T08:08:34.635088Z","iopub.status.idle":"2022-07-09T08:08:34.91177Z","shell.execute_reply.started":"2022-07-09T08:08:34.635056Z","shell.execute_reply":"2022-07-09T08:08:34.910734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Deep Fake Video Classification","metadata":{}},{"cell_type":"markdown","source":"## Importing required libraries","metadata":{}},{"cell_type":"code","source":"!pip install -U --upgrade tensorflow","metadata":{"execution":{"iopub.status.busy":"2022-07-09T09:47:38.784345Z","iopub.execute_input":"2022-07-09T09:47:38.784663Z","iopub.status.idle":"2022-07-09T09:49:22.605841Z","shell.execute_reply.started":"2022-07-09T09:47:38.784606Z","shell.execute_reply":"2022-07-09T09:49:22.60461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow_docs.vis import embed\nfrom tensorflow import keras\n#from imutils import paths\n\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport pandas as pd\nimport numpy as np\nimport imageio\nimport cv2\nimport os","metadata":{"execution":{"iopub.status.busy":"2022-07-09T09:57:47.261596Z","iopub.execute_input":"2022-07-09T09:57:47.261902Z","iopub.status.idle":"2022-07-09T09:57:47.288919Z","shell.execute_reply.started":"2022-07-09T09:57:47.261851Z","shell.execute_reply":"2022-07-09T09:57:47.287364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Visualisation","metadata":{}},{"cell_type":"code","source":"DATA_FOLDER = '../input/deepfake-detection-challenge'\nTRAIN_SAMPLE_FOLDER = 'train_sample_videos'\nTEST_FOLDER = 'test_videos'\n\nprint(f\"Train samples: {len(os.listdir(os.path.join(DATA_FOLDER, TRAIN_SAMPLE_FOLDER)))}\")\nprint(f\"Test samples: {len(os.listdir(os.path.join(DATA_FOLDER, TEST_FOLDER)))}\")","metadata":{"execution":{"iopub.status.busy":"2022-07-09T09:50:27.711837Z","iopub.execute_input":"2022-07-09T09:50:27.712461Z","iopub.status.idle":"2022-07-09T09:50:28.045889Z","shell.execute_reply.started":"2022-07-09T09:50:27.712158Z","shell.execute_reply":"2022-07-09T09:50:28.044985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_sample_metadata = pd.read_json('../input/deepfake-detection-challenge/train_sample_videos/metadata.json').T\ntrain_sample_metadata.head()","metadata":{"execution":{"iopub.status.busy":"2022-07-09T09:50:31.423939Z","iopub.execute_input":"2022-07-09T09:50:31.424304Z","iopub.status.idle":"2022-07-09T09:50:31.766964Z","shell.execute_reply.started":"2022-07-09T09:50:31.42424Z","shell.execute_reply":"2022-07-09T09:50:31.76627Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_sample_metadata.groupby('label')['label'].count().plot(figsize=(15, 5), kind='bar', title='Distribution of Labels in the Training Set')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-09T08:20:28.226698Z","iopub.execute_input":"2022-07-09T08:20:28.227009Z","iopub.status.idle":"2022-07-09T08:20:28.564649Z","shell.execute_reply.started":"2022-07-09T08:20:28.226959Z","shell.execute_reply":"2022-07-09T08:20:28.563886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_sample_metadata.shape","metadata":{"execution":{"iopub.status.busy":"2022-07-09T08:21:47.624267Z","iopub.execute_input":"2022-07-09T08:21:47.624655Z","iopub.status.idle":"2022-07-09T08:21:47.632176Z","shell.execute_reply.started":"2022-07-09T08:21:47.624595Z","shell.execute_reply":"2022-07-09T08:21:47.631209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Fake videos","metadata":{}},{"cell_type":"code","source":"fake_train_sample_video = list(train_sample_metadata.loc[train_sample_metadata.label=='FAKE'].sample(3).index)\nfake_train_sample_video","metadata":{"execution":{"iopub.status.busy":"2022-07-09T08:23:07.362913Z","iopub.execute_input":"2022-07-09T08:23:07.363259Z","iopub.status.idle":"2022-07-09T08:23:07.372654Z","shell.execute_reply.started":"2022-07-09T08:23:07.363203Z","shell.execute_reply":"2022-07-09T08:23:07.371698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def display_image_from_video(video_path):\n    capture_image = cv2.VideoCapture(video_path) \n    ret, frame = capture_image.read()\n    fig = plt.figure(figsize=(10,10))\n    ax = fig.add_subplot(111)\n    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n    ax.imshow(frame)","metadata":{"execution":{"iopub.status.busy":"2022-07-09T08:24:04.109114Z","iopub.execute_input":"2022-07-09T08:24:04.10951Z","iopub.status.idle":"2022-07-09T08:24:04.117229Z","shell.execute_reply.started":"2022-07-09T08:24:04.109445Z","shell.execute_reply":"2022-07-09T08:24:04.116463Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for video_file in fake_train_sample_video:\n    display_image_from_video(os.path.join(DATA_FOLDER, TRAIN_SAMPLE_FOLDER, video_file))","metadata":{"execution":{"iopub.status.busy":"2022-07-09T08:24:05.733174Z","iopub.execute_input":"2022-07-09T08:24:05.73378Z","iopub.status.idle":"2022-07-09T08:24:07.365205Z","shell.execute_reply.started":"2022-07-09T08:24:05.733722Z","shell.execute_reply":"2022-07-09T08:24:07.364449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Real Videos","metadata":{}},{"cell_type":"code","source":"real_train_sample_video = list(train_sample_metadata.loc[train_sample_metadata.label=='REAL'].sample(3).index)\nreal_train_sample_video","metadata":{"execution":{"iopub.status.busy":"2022-07-09T08:25:02.930732Z","iopub.execute_input":"2022-07-09T08:25:02.931034Z","iopub.status.idle":"2022-07-09T08:25:02.93917Z","shell.execute_reply.started":"2022-07-09T08:25:02.930981Z","shell.execute_reply":"2022-07-09T08:25:02.937796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for video_file in real_train_sample_video:\n    display_image_from_video(os.path.join(DATA_FOLDER, TRAIN_SAMPLE_FOLDER, video_file))","metadata":{"execution":{"iopub.status.busy":"2022-07-09T08:25:55.295492Z","iopub.execute_input":"2022-07-09T08:25:55.295792Z","iopub.status.idle":"2022-07-09T08:25:57.080227Z","shell.execute_reply.started":"2022-07-09T08:25:55.295748Z","shell.execute_reply":"2022-07-09T08:25:57.079434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Videos with same original","metadata":{}},{"cell_type":"code","source":"train_sample_metadata['original'].value_counts()[0:5]","metadata":{"execution":{"iopub.status.busy":"2022-07-09T08:27:47.665283Z","iopub.execute_input":"2022-07-09T08:27:47.665706Z","iopub.status.idle":"2022-07-09T08:27:47.678244Z","shell.execute_reply.started":"2022-07-09T08:27:47.665643Z","shell.execute_reply":"2022-07-09T08:27:47.677316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We pick one of the originals with largest number of samples.\n\nWe also modify our visualization function to work with multiple images.","metadata":{}},{"cell_type":"code","source":"def display_image_from_video_list(video_path_list, video_folder=TRAIN_SAMPLE_FOLDER):\n    plt.figure()\n    fig, ax = plt.subplots(2,3,figsize=(16,8))\n    # we only show images extracted from the first 6 videos\n    for i, video_file in enumerate(video_path_list[0:6]):\n        video_path = os.path.join(DATA_FOLDER, video_folder,video_file)\n        capture_image = cv2.VideoCapture(video_path) \n        ret, frame = capture_image.read()\n        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n        ax[i//3, i%3].imshow(frame)\n        ax[i//3, i%3].set_title(f\"Video: {video_file}\")\n        ax[i//3, i%3].axis('on')\n","metadata":{"execution":{"iopub.status.busy":"2022-07-09T08:28:44.702008Z","iopub.execute_input":"2022-07-09T08:28:44.702345Z","iopub.status.idle":"2022-07-09T08:28:44.712022Z","shell.execute_reply.started":"2022-07-09T08:28:44.702288Z","shell.execute_reply":"2022-07-09T08:28:44.710848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"same_original_fake_train_sample_video = list(train_sample_metadata.loc[train_sample_metadata.original=='atvmxvwyns.mp4'].index)\ndisplay_image_from_video_list(same_original_fake_train_sample_video)","metadata":{"execution":{"iopub.status.busy":"2022-07-09T08:29:33.783363Z","iopub.execute_input":"2022-07-09T08:29:33.78375Z","iopub.status.idle":"2022-07-09T08:29:36.262689Z","shell.execute_reply.started":"2022-07-09T08:29:33.783696Z","shell.execute_reply":"2022-07-09T08:29:36.261949Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Test video files","metadata":{}},{"cell_type":"code","source":"test_videos = pd.DataFrame(list(os.listdir(os.path.join(DATA_FOLDER, TEST_FOLDER))), columns=['video'])","metadata":{"execution":{"iopub.status.busy":"2022-07-09T09:56:50.096013Z","iopub.execute_input":"2022-07-09T09:56:50.096374Z","iopub.status.idle":"2022-07-09T09:56:50.109341Z","shell.execute_reply.started":"2022-07-09T09:56:50.096299Z","shell.execute_reply":"2022-07-09T09:56:50.108518Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_videos.head()","metadata":{"execution":{"iopub.status.busy":"2022-07-09T08:31:55.316498Z","iopub.execute_input":"2022-07-09T08:31:55.316823Z","iopub.status.idle":"2022-07-09T08:31:55.326777Z","shell.execute_reply.started":"2022-07-09T08:31:55.316769Z","shell.execute_reply":"2022-07-09T08:31:55.325646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"visual of a videos.","metadata":{}},{"cell_type":"code","source":"display_image_from_video(os.path.join(DATA_FOLDER, TEST_FOLDER, test_videos.iloc[2].video))","metadata":{"execution":{"iopub.status.busy":"2022-07-09T08:32:25.934158Z","iopub.execute_input":"2022-07-09T08:32:25.934484Z","iopub.status.idle":"2022-07-09T08:32:26.550432Z","shell.execute_reply.started":"2022-07-09T08:32:25.934408Z","shell.execute_reply":"2022-07-09T08:32:26.549251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Play video files","metadata":{}},{"cell_type":"code","source":"fake_videos = list(train_sample_metadata.loc[train_sample_metadata.label=='FAKE'].index)","metadata":{"execution":{"iopub.status.busy":"2022-07-09T09:59:02.695259Z","iopub.execute_input":"2022-07-09T09:59:02.695558Z","iopub.status.idle":"2022-07-09T09:59:02.701653Z","shell.execute_reply.started":"2022-07-09T09:59:02.695509Z","shell.execute_reply":"2022-07-09T09:59:02.700545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.display import HTML\nfrom base64 import b64encode\n\ndef play_video(video_file, subset=TRAIN_SAMPLE_FOLDER):\n    video_url = open(os.path.join(DATA_FOLDER, subset,video_file),'rb').read()\n    data_url = \"data:video/mp4;base64,\" + b64encode(video_url).decode()\n    return HTML(\"\"\"<video width=500 controls><source src=\"%s\" type=\"video/mp4\"></video>\"\"\" % data_url)\n\nplay_video(fake_videos[10])","metadata":{"execution":{"iopub.status.busy":"2022-07-09T09:59:04.228199Z","iopub.execute_input":"2022-07-09T09:59:04.228515Z","iopub.status.idle":"2022-07-09T09:59:04.75913Z","shell.execute_reply.started":"2022-07-09T09:59:04.228461Z","shell.execute_reply":"2022-07-09T09:59:04.758024Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Modelling","metadata":{}},{"cell_type":"markdown","source":"### A CNN-RNN Architecture","metadata":{}},{"cell_type":"code","source":"IMG_SIZE = 224\nBATCH_SIZE = 64\nEPOCHS = 10\n\nMAX_SEQ_LENGTH = 20\nNUM_FEATURES = 2048","metadata":{"execution":{"iopub.status.busy":"2022-07-09T09:50:41.78002Z","iopub.execute_input":"2022-07-09T09:50:41.780332Z","iopub.status.idle":"2022-07-09T09:50:41.785027Z","shell.execute_reply.started":"2022-07-09T09:50:41.780278Z","shell.execute_reply":"2022-07-09T09:50:41.783976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def crop_center_square(frame):\n    y, x = frame.shape[0:2]\n    min_dim = min(y, x)\n    start_x = (x // 2) - (min_dim // 2)\n    start_y = (y // 2) - (min_dim // 2)\n    return frame[start_y : start_y + min_dim, start_x : start_x + min_dim]\n\n\ndef load_video(path, max_frames=0, resize=(IMG_SIZE, IMG_SIZE)):\n    cap = cv2.VideoCapture(path)\n    frames = []\n    try:\n        while True:\n            ret, frame = cap.read()\n            if not ret:\n                break\n            frame = crop_center_square(frame)\n            frame = cv2.resize(frame, resize)\n            frame = frame[:, :, [2, 1, 0]]\n            frames.append(frame)\n\n            if len(frames) == max_frames:\n                break\n    finally:\n        cap.release()\n    return np.array(frames)","metadata":{"execution":{"iopub.status.busy":"2022-07-09T09:50:47.508499Z","iopub.execute_input":"2022-07-09T09:50:47.508826Z","iopub.status.idle":"2022-07-09T09:50:47.519075Z","shell.execute_reply.started":"2022-07-09T09:50:47.508767Z","shell.execute_reply":"2022-07-09T09:50:47.517941Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"using the InceptionV3 model to extract meaningful features from the extracted frames.","metadata":{}},{"cell_type":"code","source":"def build_feature_extractor():\n    feature_extractor = keras.applications.InceptionV3(\n        weights=\"imagenet\",\n        include_top=False,\n        pooling=\"avg\",\n        input_shape=(IMG_SIZE, IMG_SIZE, 3),\n    )\n    preprocess_input = keras.applications.inception_v3.preprocess_input\n\n    inputs = keras.Input((IMG_SIZE, IMG_SIZE, 3))\n    preprocessed = preprocess_input(inputs)\n\n    outputs = feature_extractor(preprocessed)\n    return keras.Model(inputs, outputs, name=\"feature_extractor\")\n\n\nfeature_extractor = build_feature_extractor()","metadata":{"execution":{"iopub.status.busy":"2022-07-09T09:50:51.361191Z","iopub.execute_input":"2022-07-09T09:50:51.361538Z","iopub.status.idle":"2022-07-09T09:50:54.984583Z","shell.execute_reply.started":"2022-07-09T09:50:51.361478Z","shell.execute_reply":"2022-07-09T09:50:54.983853Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prepare_all_videos(df, root_dir):\n    num_samples = len(df)\n    video_paths = list(df.index)\n    labels = df[\"label\"].values\n    labels = np.array(labels=='FAKE').astype(np.int)\n\n    # `frame_masks` and `frame_features` are what we will feed to our sequence model.\n    # `frame_masks` will contain a bunch of booleans denoting if a timestep is\n    # masked with padding or not.\n    frame_masks = np.zeros(shape=(num_samples, MAX_SEQ_LENGTH), dtype=\"bool\")\n    frame_features = np.zeros(\n        shape=(num_samples, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n    )\n\n    # For each video.\n    for idx, path in enumerate(video_paths):\n        # Gather all its frames and add a batch dimension.\n        frames = load_video(os.path.join(root_dir, path))\n        frames = frames[None, ...]\n\n        # Initialize placeholders to store the masks and features of the current video.\n        temp_frame_mask = np.zeros(shape=(1, MAX_SEQ_LENGTH,), dtype=\"bool\")\n        temp_frame_features = np.zeros(\n            shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n        )\n\n        # Extract features from the frames of the current video.\n        for i, batch in enumerate(frames):\n            video_length = batch.shape[0]\n            length = min(MAX_SEQ_LENGTH, video_length)\n            for j in range(length):\n                temp_frame_features[i, j, :] = feature_extractor.predict(\n                    batch[None, j, :]\n                )\n            temp_frame_mask[i, :length] = 1  # 1 = not masked, 0 = masked\n\n        frame_features[idx,] = temp_frame_features.squeeze()\n        frame_masks[idx,] = temp_frame_mask.squeeze()\n\n    return (frame_features, frame_masks), labels","metadata":{"execution":{"iopub.status.busy":"2022-07-09T09:51:01.453931Z","iopub.execute_input":"2022-07-09T09:51:01.45424Z","iopub.status.idle":"2022-07-09T09:51:01.467342Z","shell.execute_reply.started":"2022-07-09T09:51:01.45418Z","shell.execute_reply":"2022-07-09T09:51:01.46643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nTrain_set, Test_set = train_test_split(train_sample_metadata,test_size=0.1,random_state=42,stratify=train_sample_metadata['label'])\n\nprint(Train_set.shape, Test_set.shape )","metadata":{"execution":{"iopub.status.busy":"2022-07-09T09:51:05.713246Z","iopub.execute_input":"2022-07-09T09:51:05.713571Z","iopub.status.idle":"2022-07-09T09:51:06.599083Z","shell.execute_reply.started":"2022-07-09T09:51:05.713514Z","shell.execute_reply":"2022-07-09T09:51:06.598314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data, train_labels = prepare_all_videos(Train_set, \"train\")\ntest_data, test_labels = prepare_all_videos(Test_set, \"test\")\n\nprint(f\"Frame features in train set: {train_data[0].shape}\")\nprint(f\"Frame masks in train set: {train_data[1].shape}\")","metadata":{"execution":{"iopub.status.busy":"2022-07-09T09:51:13.648645Z","iopub.execute_input":"2022-07-09T09:51:13.64895Z","iopub.status.idle":"2022-07-09T09:51:13.774896Z","shell.execute_reply.started":"2022-07-09T09:51:13.648898Z","shell.execute_reply":"2022-07-09T09:51:13.77412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## The sequence model","metadata":{}},{"cell_type":"code","source":"frame_features_input = keras.Input((MAX_SEQ_LENGTH, NUM_FEATURES))\nmask_input = keras.Input((MAX_SEQ_LENGTH,), dtype=\"bool\")\n\n#can use LSTM as well\nx = keras.layers.GRU(16, return_sequences=True)(\n    frame_features_input, mask=mask_input\n)\nx = keras.layers.GRU(8)(x)\nx = keras.layers.Dropout(0.4)(x)\nx = keras.layers.Dense(8, activation=\"relu\")(x)\noutput = keras.layers.Dense(1, activation=\"sigmoid\")(x)\n\nmodel = keras.Model([frame_features_input, mask_input], output)\n\nmodel.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2022-07-09T09:51:20.832718Z","iopub.execute_input":"2022-07-09T09:51:20.833031Z","iopub.status.idle":"2022-07-09T09:51:22.303145Z","shell.execute_reply.started":"2022-07-09T09:51:20.832975Z","shell.execute_reply":"2022-07-09T09:51:22.302474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"checkpoint = keras.callbacks.ModelCheckpoint('./', save_weights_only=True, save_best_only=True)\nhistory = model.fit(\n        [train_data[0], train_data[1]],\n        train_labels,\n        validation_data=([test_data[0], test_data[1]],test_labels),\n        callbacks=[checkpoint],\n        epochs=EPOCHS,\n        batch_size=8\n    )","metadata":{"execution":{"iopub.status.busy":"2022-07-09T09:51:56.702116Z","iopub.execute_input":"2022-07-09T09:51:56.70248Z","iopub.status.idle":"2022-07-09T09:52:19.310748Z","shell.execute_reply.started":"2022-07-09T09:51:56.702422Z","shell.execute_reply":"2022-07-09T09:52:19.309733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Inference","metadata":{}},{"cell_type":"code","source":"def prepare_single_video(frames):\n    frames = frames[None, ...]\n    frame_mask = np.zeros(shape=(1, MAX_SEQ_LENGTH,), dtype=\"bool\")\n    frame_features = np.zeros(shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\")\n\n    for i, batch in enumerate(frames):\n        video_length = batch.shape[0]\n        length = min(MAX_SEQ_LENGTH, video_length)\n        for j in range(length):\n            frame_features[i, j, :] = feature_extractor.predict(batch[None, j, :])\n        frame_mask[i, :length] = 1  # 1 = not masked, 0 = masked\n\n    return frame_features, frame_mask\n\ndef sequence_prediction(path):\n    frames = load_video(os.path.join(DATA_FOLDER, TEST_FOLDER,path))\n    frame_features, frame_mask = prepare_single_video(frames)\n    return model.predict([frame_features, frame_mask])[0]\n    \n# This utility is for visualization.\n# Referenced from:\n# https://www.tensorflow.org/hub/tutorials/action_recognition_with_tf_hub\ndef to_gif(images):\n    converted_images = images.astype(np.uint8)\n    imageio.mimsave(\"animation.gif\", converted_images, fps=10)\n    return embed.embed_file(\"animation.gif\")\n\n\ntest_video = np.random.choice(test_videos[\"video\"].values.tolist())\nprint(f\"Test video path: {test_video}\")\n\nif(sequence_prediction(test_video)>=0.5):\n    print(f'The video is FAKE')\nelse:\n    print(f'The video is REAL')\n\nplay_video(test_video,TEST_FOLDER)","metadata":{"execution":{"iopub.status.busy":"2022-07-09T10:03:36.501113Z","iopub.execute_input":"2022-07-09T10:03:36.501445Z","iopub.status.idle":"2022-07-09T10:03:43.776798Z","shell.execute_reply.started":"2022-07-09T10:03:36.501395Z","shell.execute_reply":"2022-07-09T10:03:43.775809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Conclusion","metadata":{}},{"cell_type":"markdown","source":"**The deep fake image classifier and deep fake video classifier model gave a generalisation error of around ~82%**","metadata":{}},{"cell_type":"markdown","source":"# Reference","metadata":{}},{"cell_type":"markdown","source":"https://keras.io/examples/vision/video_classification/\n\nhttps://www.kaggle.com/code/gpreda/deepfake-starter-kit\n\nhttps://www.kaggle.com/code/robikscube/kaggle-deepfake-detection-introduction\n\nhttps://www.kaggle.com/code/humananalog/binary-image-classifier-training-demo\n\nhttps://www.kaggle.com/datasets/dagnelies/deepfake-faces\n\nhttps://www.kaggle.com/code/gautam20bce1227/fake-detection-on-images","metadata":{}}]}